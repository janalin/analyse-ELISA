[
["index.html", "Introduction to Model Selection Preface Outline", " Introduction to Model Selection 2018-05-22 Preface The purpose of this document is to summarise the concepts discussed in the model selection reading club – taking place in the Computational Systems Biology group at the D-BSSE (ETH Zurich) in Winter/Spring 2018. The reading club is split into two groups: One group covers the introduction into the topic and discusses the commonly used selection criteria Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC). The second group discusses their extensions for mixed-effects models. This document summerises the topics of both groups. The current structure of the document is shown below but might change during the course of the reading club. Outline Introduction The Bias-Variance Tradeoff Model Selection Criteria In general For Linear Mixed Models (LMEs) For Non-Linear Mixed Models (NLMEs) "],
["intro.html", "1 Introduction", " 1 Introduction The goal of model selection is to identify the best model among a set of competing models. What exactly means the best model? Usually, we consider the best model to be the simplest of the best fitting models, however, it is not exactly clear what the simplest or the best fitting model means. Consider for instance the following example: Figure 1.1: Example problem: which model is the best model? Here, \\(k\\) corresponds to the number of parameters (including the intercept). The sum over all squared residuals is given by \\(SSE\\) (sum of squared error): \\[SSE = \\sum_{i = 1}^n \\left( y(x_i) - \\hat{f}(x_i) \\right) ^ 2 = \\sum_{i = 1}^n \\underbrace{ \\left(\\underbrace{\\underbrace{y_i}_{\\text{observed}} - \\underbrace{\\hat{y}_i}_{\\text{predicted}}}_{\\text{residual } r_i}\\right)^2}_{\\text{squared error}}\\], where \\(n\\) corresponds to the number of observations (here \\(n = 20\\)), \\(y_i\\) is the observed response for given \\(x_i\\) and \\(\\hat{f}(x_i)\\) is the predicted reponse of model \\(\\hat{f}\\) for a given \\(x_i\\). Clearly, as more parameters the model has as better the model fits the data and as smaller is the sum of squared errors. But which of the three models is the best model? When does a better fit justifies more parameters or a more complex model? To answer this question, we first must decide on how to asses the quality of fit (is \\(SSE\\) the appropriate?) and how to measure model complexity. How to asses the quality of fit is the topic of the next chapter. Model complexity might be covered in another chapter. "],
["bias-var.html", "2 The Bias-Variance Tradeoff 2.1 The quality of fit in regression 2.2 Simulation study 2.3 Error decomposition 2.4 More general: the loss function 2.5 The training error 2.6 The principle of regularization", " 2 The Bias-Variance Tradeoff 2.1 The quality of fit in regression Quality of fit means usually how well our model \\(\\hat{f}\\) matches the experimental observations \\(y\\). Or more precisely, how much the predicted response value for a given observation \\(\\hat{f}(x_i) = \\hat{y}_i\\) is close to the observed response value \\(y(x_i)\\) for that observation \\(x_i\\). The \\(SSE\\) introduced in the previous section is not applicable because it is sample size dependent: as more observations we have, as higher the \\(SSE\\). Thus, we need to normalize for the number of observation \\(n\\). This gives us the most-commonly used measure for the quality of fit in regression analysis – the mean squared error \\(MSE\\): \\[MSE = \\dfrac{1}{n} SSE = \\dfrac{1}{n} \\sum_{i = 1}^n \\left( y(x_i) - \\hat{f}(x_i) \\right) ^ 2 = \\dfrac{1}{n} \\sum_{i = 1}^n \\underbrace{ \\left(\\underbrace{\\underbrace{y_i}_{\\text{observed}} - \\underbrace{\\hat{y}_i}_{\\text{predicted}}}_{\\text{residual } r_i}\\right)^2}_{\\text{squared error}} \\ , \\] The MSE is the total mean error between our actual observations and model prediction. In the following section, we will see that there are different error sources that contribute to the total error (\\(SSE\\) and \\(MSE\\)). 2.2 Simulation study We perform a simulation study with the four example models introduced in section 1. \\[\\begin{align} \\hat{f}_1 (x) &amp;= \\beta_0 + \\beta_1 \\ x &amp;\\text{Polynomial of degree 1 (linear model)} \\\\ \\hat{f}_2 (x) &amp;= \\beta_0 + \\beta_1 \\ x + \\beta_2 \\ x^2 &amp;\\text{Polynomial of degree 2 (quadratic model)} \\\\ \\hat{f}_3 (x) &amp;= \\beta_0 + \\beta_1 \\ x + \\beta_2 \\ x^2 + \\ldots + \\beta_{16} \\ x^{15} &amp; \\text{Polynomial of degree 15} \\\\ \\hat{f}_4 (x) &amp;= \\beta_0 + \\beta_1 \\ exp(x) &amp; \\text{Exponential model} \\\\ \\end{align}\\] All models share the same vector of independent variables \\(x\\). Note, that model \\(\\hat{f}_{3}\\) can include models \\(\\hat{f}_{i&lt;3}\\) (and \\(\\hat{f}_{2}\\) can include \\(\\hat{f}_{1}\\)), while model \\(\\hat{f}_4\\) belongs to a different category. We will see why this is important. Assume you have a set of observations \\(y\\) given by some data-generating process for given values of \\(x\\). For instance, \\(x\\) might be a set of experimental conditions or a set of patient biomarkers. Then \\(y\\) might correspond to a fluorescence signal under the condition \\(x\\) or a disease outcome in the considered patient population. Note, that you do not observe the full population from which \\(y\\) is drawn but only a sample; performing another experiment under conditions \\(x\\) or collecting more patient samples for the considered patient population corresponds ideally to taking a new random sample from the same population. We try to design our experiments and studies such that we can assume that this is the case, otherwise we cannot use the discussed model selection criteria, because then we cannot assume that the observations come from the same data-generating process and then they can not be explained by the same model \\(\\hat{f}\\). We assume there is some real unknown model \\(f\\) that gives rise to our observations \\(y\\): \\[ y = f(x) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2) \\] We fit the four models to our available obseravtions \\(y\\). The summary statistics is shown in Table below. get.statinfo &lt;- function(model.list, model.titles, idx) { tibble( Model = model.titles[idx], k = length(model.list[[idx]]$coefficients), n = nobs(model.list[[idx]]), df = df.residual(model.list[[idx]]), # n - k SSE = round(sum(model.list[[idx]]$residuals ^ 2), 0), MSE = round((sum(model.list[[idx]]$residuals ^ 2)) / nobs(model.list[[idx]]), 0) ) } statinfo &lt;- purrr::map( seq(4), .f = function(.) get.statinfo(model.list, model.titles, .) ) %&gt;% do.call(&quot;rbind&quot;, .) knitr::kable( statinfo, caption = &#39;Summary statistics for fitting the four example models to the training data.&#39;, booktabs = TRUE ) Table 2.1: Summary statistics for fitting the four example models to the training data. Model k n df SSE MSE Linear model 2 20 18 89005 4450 Quadratic model 3 20 17 26197 1310 Polynomial of degree 15 16 20 4 8700 435 Exponential model 2 20 18 638936 31947 The \\(MSE\\) between model and training data is often called training \\(MSE\\). But to asses the model performance we are usually not interested in the training \\(MSE\\). Instead, we are interested in how well our model performs on new observations – the test \\(MSE\\). Let us compute the test \\(MSE\\) of the four example models for 100 new observations. Here, a new observation means a new sample drawn from the same population of observations as the training data set \\(y\\). .compute.test.MSE &lt;- function(model.list, seed, idx) { set.seed(seed) test.data &lt;- tibble::tibble(x = seq(20), y = jitter(2 * x ^ 2, amount = 60)) pred.y &lt;- c(predict.lm(model.list[[idx]], test.data)) test.MSE &lt;- round(mean((test.data$y - pred.y) ^ 2),0) return(test.MSE) } # compute new test MSEs n.models &lt;- 4 n.obs &lt;- 100 test.MSEs &lt;- purrr::map( seq(n.models), .f = function(.) lapply(seq(n.obs), function(x) .compute.test.MSE(model.list, x, .)) %&gt;% do.call(&quot;rbind&quot;, .) ) # convert list to tibble df.test.MSEs &lt;- test.MSEs %&gt;% do.call(&quot;cbind&quot;, .) %&gt;% magrittr::set_colnames(model.titles) %&gt;% tibble::as.tibble(.) %&gt;% dplyr::mutate(Type = &#39;Test MSE&#39;) # plot results .plot.hist &lt;- function(df, model.titles, idx.model) { ggplot2::ggplot() + geom_histogram( aes(x = df[[idx.model]]), color = &quot;black&quot;, fill = &quot;white&quot;, bins = 20) + theme_book() + labs(title = model.titles[idx.model], x = df$Type, y = &quot;Frequency&quot;) + geom_vline( xintercept = median(df[[idx.model]]), lty = 3, color = &quot;red&quot;, lwd = 1.2) + ylim(0, 15) } hist.MSEs &lt;- purrr::map( seq(n.models), .f = function(.).plot.hist(df.test.MSEs, model.titles, .) ) # show plots invisible(lapply(hist.MSEs, function(x) methods::show(x))) ## Warning: Removed 1 rows containing missing values (geom_bar). Figure 2.1: Histogram of test MSEs computed on 100 new observations. Now, let us compute the training \\(MSE\\) for 100 new observations and compare it to the test \\(MSE\\). .fit.new.obs &lt;- function(model.list, seed){ set.seed(seed) test.data &lt;- tibble::tibble(x = seq(20), y = jitter(2 * x ^ 2, amount = 60)) new.fit &lt;- purrr::map( seq(length(model.list)), .f = function(.) lm(formula(model.list[[.]]), test.data) ) return(new.fit) } .get.MSE &lt;- function(list.of.fits, idx.fit, idx.model){ MSE = round((sum(list.of.fits[[idx.fit]][[idx.model]]$residuals ^ 2)) / nobs(list.of.fits[[idx.fit]][[idx.model]]), 0) return(MSE) } # fit models to new obserations and store in list n.obs &lt;- 100 n.models &lt;- 4 # we fit the four models new.fits &lt;- purrr::map( seq(n.obs), .f = function(.) .fit.new.obs(model.list, .) ) # get new training MSEs training.MSEs &lt;- purrr::map( seq(n.models), .f = function(.) lapply(seq(n.obs), function(x) .get.MSE(new.fits, x, .)) %&gt;% do.call(&quot;rbind&quot;, .) ) # convert list to tibble and merge with df.test.MSEs df.MSEs &lt;- training.MSEs %&gt;% do.call(&quot;cbind&quot;, .) %&gt;% magrittr::set_colnames(model.titles) %&gt;% as.tibble(.) %&gt;% dplyr::mutate(Type = &#39;Training MSE&#39;) %&gt;% dplyr::bind_rows(df.test.MSEs) %&gt;% tidyr::gather(Model, MSE, -Type) # define order of models (how they should appear in plot) df.MSEs$Model &lt;- factor(df.MSEs$Model, levels=c(model.titles[length(model.titles)], model.titles[1:length(model.titles)-1])) # plot all training MSEs and test MSEs ggplot2::ggplot(df.MSEs, aes(x = factor(Model), y = MSE)) + scale_shape_discrete(solid=F) + geom_point(aes(group=Type, color = Type), size = 3, stroke = 1, alpha = 0.05, position = position_dodge(width = 0.5)) + geom_boxplot(aes(color=Type), width = 0.5) + labs(x = &#39;Model&#39;, y = &#39;MSE&#39;, title = &#39;&#39;) + background_grid(major= &quot;y&quot;, colour.major=&quot;grey40&quot;) + scale_y_log10( breaks = scales::trans_breaks(&quot;log10&quot;, function(x) 10^x), labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x))) + annotation_logticks(sides = &#39;lr&#39;) + theme_book() + panel_border() + scale_color_brewer(palette = &quot;Set1&quot;) + scale_x_discrete(labels = c(&quot;Exp.&quot;, &quot;Linear&quot;, &quot;Quadr.&quot;, &quot;Polyn. 15&quot;)) Figure 2.2: Comparison of training and test MSEs computed on 100 new data sets. The polynomial of degree 15 has the best training \\(MSE\\) but the worse test \\(MSE\\). This behavior is often called overfitting. In the next section we will show that this results from high model variance (error due to estimation) and low bias (error due to approximation). The linear model performs bad on both data sets. Note that the \\(MSE\\) of the linear model does not change as as much as for \\(\\hat{f}_3\\) – this indicates that the model variance of \\(\\hat{f}_1\\) is rather low and that the high \\(MSE\\) results from high bias. The quadratic model performs well on both data sets suggesting a low model variance and a low bias. And indeed, this model was used to generate the data for this example. 2.3 Error decomposition [work in progress] Assume we know the real model \\(f\\) that gives rise to some real observations \\(y\\) for values \\(x\\) described by \\(y = f(x) + \\epsilon\\), with random noise \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). We are interested in the quality of fit for a given model \\(\\hat{f}\\). It can be shown, that the expected test \\(MSE\\) for \\(\\hat{f}(x_i)\\) can always be decomposed into the sum of the following three components: \\[ E \\left( y(x_i) - \\hat{f}(x_i) \\right)^2 = \\mathrm{Var}(\\hat{f}(x_i)) + \\left( \\mathrm{Bias} (\\hat{f}(x_i)) \\right)^2 + \\mathrm{Var}(\\epsilon)\\] Variance of the fitted model: \\(\\mathrm{Var}(\\hat{f}(x_i)) = E(\\hat{f}(x_i)^2) - E(\\hat{f}(x_i))^2\\) The variance of \\(\\hat{f}(x_i)\\) refers to the amount by which model \\(\\hat{f}\\) would change, if we estimated it using a different training data set (changes in estimated parameters); in general, more flexible models show higher variance than less flexible models. This error term is also called error due to estimation. Figure 2.3: Visualization of model variance: The example models were fitted to four different testing data sets. Squared bias of the fitted model: \\(\\left( \\mathrm{Bias} (\\hat{f}(x_i)) \\right)^2 = \\left( E\\left(\\hat{f}(x_i)\\right) - f(x_i) \\right)^2\\) The squared bias of \\(\\hat{f}(x_i)\\) is also called error due to approximation. More flexible models result in lower bias (but higher variance), while less flexible models result in high bias (and lower variance). Variance due to measurement noise: \\(\\mathrm{Var}(\\epsilon) = \\sigma^2\\) The variance of the error terms, which is irreducible. 2.4 More general: the loss function In the previous section, we used the mean squared error to measure error between data \\(y\\) and model prediction \\(\\hat{f}(x) = \\hat{y}\\). We can generalize this concept to other functions using the so-called loss function, denoted by \\(L(y,\\hat{y})\\). In regression, possible loss functions are: squared error function : \\(L(y,\\hat{y}) = \\left( y - \\hat{y} \\right)^2\\) absolute error function: \\(L(y,\\hat{y} = \\bigg| y - \\hat{y} \\bigg|\\) Minimizing the squared error gives the conditional mean and is easier to solve, because the derivates are continuous and there exists a closed form solution for setting the derivates to zero. Minimizing the absolute error requires an iterative approach and gives the conditional median. The squared error penalizes large errors more than the absolute error. In other words, the absolute error is more robust to outliers. In classification problems, we have a categorical response \\(g \\in \\lbrace{ 1, \\ldots, K \\rbrace}\\) and predicted category \\(\\hat{f}(x) = \\hat{g}\\). A classifier gives either \\(\\hat{g}\\) directly as in \\(k\\)-nearest neighbor classification; or the classifier models the probabilities \\(p_{k} (x) = \\mathrm{Pr}(g = k | x)\\) or some monotone transformation \\(f_k(x)\\). The predicted category is then given by \\(\\hat{g} = \\mathrm{ arg \\ max}_k \\hat{p}_k (x)\\) 0-1 loss function for categorical response \\(g\\) and predicted category \\(\\hat{g}\\), which is the indicator function \\(I(g \\neq \\hat{g})\\) where the error is zero, if the classification is perfect: \\[ L(g,\\hat{g} = I(g \\neq \\hat{g}) = \\begin{cases} 0, &amp; \\text{if $g =\\hat{g}$}.\\\\ 1, &amp; \\text{otherwise}. \\end{cases} \\] The 0-1 loss function, however, is not differentiable and not convex. T 2.5 The training error The training error can then be defined as the average loss over the training sample: \\[ \\bar{err}= \\dfrac{1}{n} \\sum_{i = 1}^n L \\left ( y(x_i) ,\\hat{f}(x_i) \\right) \\] 2.6 The principle of regularization 2.6.1 Ridge and lasso regression The optimization problem in a basic regression is to find \\(\\hat{\\beta}\\) such that: \\[ \\hat{\\beta}=\\underset{\\beta}{argmin} L(y,f(x,\\beta))\\] One problem is that if the coefficients of \\(\\beta\\) are unconstrained in the classical linear regression, they can tend to have high values and this results in a high variance. The idea is then to penalize for high values of the coefficients of \\(\\beta\\) using penalized regression. The optimization problem tries to find \\(\\hat{\\beta}\\) such that: \\[ \\hat{\\beta}=\\underset{\\beta}{argmin} (L(y,f(x,\\beta))+\\lambda J(\\beta))\\] Where: the ridge constraint: \\(J(\\beta)=\\lVert \\beta \\rVert_{2}^{2}\\) the lasso constraint: \\(J(\\beta)=\\lVert \\beta \\rVert_{1}\\) the elastic net constraint: \\(J(\\beta)=\\lambda_1\\lVert \\beta \\rVert_{1}+\\lambda_2\\lVert \\beta \\rVert_{2}^{2}\\) The ridge constraint thus penalizes the large coefficients (this is called regularization). Similarly, the lasso constraint penalizes high values of the coefficients, but also enforces the \\(\\beta\\) coefficients to be zero. Figure 2.4: Contours of the error and constraint functions for the lasso (left) and ridge (right) regression. The solid blue areas are the constraint regions, while the red ellipses are the contours of the residual sum of squares. Image taken from James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013. The \\(\\lambda\\) value is called the shrinkage parameter, and for each \\(\\lambda\\), we will have a solution. It controls: The size of the coefficients The amount of regularization As \\(\\lambda\\) tends towards zero, we obtain the classical regression Usually the \\(\\lambda\\) parameter is chosen by testing at different values of \\(\\lambda\\) and looking at the solution. 2.6.2 Connection to the bias-variance tradeoff The principle of regularization is then to introduce a bias by having simpler models, so that the variance is reduced. Indeed simpler models tend to have smaller variance in further tests. This is worthwhile only if the decrease in variance exceeds the increase in bias. "],
["training-error-rate.html", "3 The Training Error Rate", " 3 The Training Error Rate "],
["in-sample-prediction-error.html", "4 Estimating the In-Sample Prediciton Error", " 4 Estimating the In-Sample Prediciton Error "],
["effective-params.html", "5 The Effective Number of Parameters", " 5 The Effective Number of Parameters "]
]
